{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde0fffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser \n",
    "import datetime \n",
    "import os \n",
    "import pickle\n",
    "import time \n",
    "from io import StringIO \n",
    "from collections import defaultdict\n",
    "import json\n",
    "import docx\n",
    "import flask\n",
    "import flask_restplus\n",
    "import gensim.summarization\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import spacy\n",
    "import torch\n",
    "from OpenSSL import SSL\n",
    "from PIL import Image\n",
    "from benfordslaw import benfordslaw\n",
    "from bs4 import BeautifulSoup\n",
    "from commonregex import CommonRegex\n",
    "from flask import Flask, request, jsonify\n",
    "from flask_cors import CORS\n",
    "from flask_restplus import Resource, Api \n",
    "from gensim.summarization.summarizer import summarize\n",
    "from langdetect import detect_langs, DetectorFactory \n",
    "from nltk.tokenize import sent_tokenize\n",
    "from pandas import read_csv\n",
    "from profanity_check import predict \n",
    "from pyod.models.knn import KNN\n",
    "from rake_nltk import Rake \n",
    "from sklearn import svm \n",
    "from sklearn.ensemble import IsolationForest\n",
    "from tika import parser, tika \n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration \n",
    "from werkzeug.datastructures import FileStorage\n",
    "from wordcloud import WordCloud, STOPWORDS,ImageColorGenerator\n",
    "\n",
    "flask_restplus.apidoc.apidoc.static_url_path = \"/static\"\n",
    "\n",
    "@Flask_restplus.apidoc.apidoc.add_app_template_global\n",
    "def swagger_static(filename):\n",
    "    return flask.url_for (\"restplus_doc static\", filename=\"swagger-vi/dist/{0}\".format(filename))\n",
    "\n",
    "igad_application = Flask(_name_)\n",
    "igad_application.config.SWAGGER_UI_DOC_EXPANSION = \"list\"\n",
    "CORS(igad_application)\n",
    "api = Api(igad_application, version=\"1.0\", title=\"Document Analyzer\", description=\"API\")\n",
    "\n",
    "def sentence_finder(text, word_list):\n",
    "        sentences = sent_tokenize(text)\n",
    "        mpa = dict.fromkeys(range (32))\n",
    "        return \" \".join([sent.translate(mpa) for sent in sentences if any(word in sent.lower() for word in word_list)])\n",
    "    \n",
    "upload_parser = api.parser()\n",
    "upload_parser.add_argument('file', location='files', required=True, type=Filestorage, help='Upload the PDF file') \n",
    "upload_parser.add_argument('Task', types='string', required=True, choices=[\"Summarization\", \"Topic\", \"Question\"], help=\"Type of task. If Task is QnA, Quesion is required. If Task is Topic Extraction, Topic is required.\")\n",
    "upload_parser.add_argument('question', type='string', required=False, help='Question')\n",
    "upload_parser.add_argument('Topic', type='string', required=False,choices=[\"PEP\", \"Terrorist Financing\", \"AML\"], help=\"Topic for which info to be extracted\")\n",
    "\n",
    "@api.route('/api/v1/DocAnalyzer')\n",
    "@api.response(500, 'An unexpected error occurred.')\n",
    "class IGAD_Document_Auditor(Resource):\n",
    "    @api.expect(upload_parser)\n",
    "    def post(self):\n",
    "        config = configparser.ConfigParser()\n",
    "        config.read('config.ini')\n",
    "        basePath = config['DEFAULT']['basePath']\n",
    "        #tika.TikaJarPath = basePath + 'tika'\n",
    "        uploaded_file = request.files['file'] \n",
    "        question = request.args.get('question')\n",
    "        print (' question::', (str(question)))\n",
    "        \n",
    "        task = request.args.get('Task')\n",
    "        topic = request.args.get ('Topic')\n",
    "        \n",
    "        st=datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d_%H-%M-%S')\n",
    "        start = time.time()\n",
    "        print ('*******Execution started::',st)\n",
    "        \n",
    "        pos = uploaded_file.filename.rfind(\",\")\n",
    "        final_filename = uploaded_file.filename[:pos].replace(\" \", \"\")+ \"_\" + st\n",
    "        \n",
    "        UPLOAD_FOLDER = basePath + 'input/'\n",
    "        uploaded_file.save(os.path.join(UPLOAD_FOLDER, final_filename + uploaded_file.filename[pos:]))\n",
    "        parsedPDF = parser.from_file(os-path.join(UPLOAD_FOLDER, final_filename + uploaded_file.filename[pos:]))\n",
    "        pdf_content = str(parsedPDF[\"content\"].encode('ascii', errors='ignore'))\n",
    "        preprocess_text = pdf _content.strip().replace(\"\\\\n\",\"\") \n",
    "\n",
    "        if task == \"Summarization\":\n",
    "            if int(parsedPDF[\"metadata\"]['xmpTPg:NPages']) < 40:\n",
    "                tokenizer = T5Tokenizer.from_pretrained(basePath + 'models/t5_small')\n",
    "                model = T5ForConditionalGeneration.from_pretrained(basePath + 'models/t5_small')\n",
    "                device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "                t5_prepared_Text = \"summarize:\" + preprocess_text\n",
    "\n",
    "                extractive_summary = summarize(preprocess_text, word_count=200)\n",
    "\n",
    "                tokenized_text = tokenizer.encode(t5_prepared_Text, return_tensors=\"pt\").to(device)\n",
    "                summary_ids = model.generate(tokenized_text, num_beans=4, no_repeat_ngram_size=2, min_length=100, max_length=400, early_stopping=False)\n",
    "                abstractive_summary_t5 = str(tokenizer.decode(summary_ids[0], skip_special_tokens=True))\n",
    "            \n",
    "                if (len(list(map(lambda x: x.strip().capitalize(), abstractive_summary_t5.split('.')))[-1]) < 40):\n",
    "                    abstractive_summary = str('. '.join(\n",
    "                    list(map(lambda x: x.strip().capitalize(), abstractive_summary_t5.split('.')))[:-1]))+ '.'\n",
    "                else:\n",
    "                    abstractive_summary= '. '.join(\n",
    "                    list(map(lambda x: x.strip().capitalize(), abstractive_summary_t5.split('.'))))\n",
    "\n",
    "                file _data_sent = {'Abstractive Summary': str(abstractive_summary),\n",
    "                           'Extractive Summary' : str(extractive_summary)}\n",
    "        \n",
    "            else:\n",
    "                if (len(preprocess_text) > 100000):\n",
    "                    compress_ratio = 0.006\n",
    "                elif(len(preprocess_text) ≤ 1000):\n",
    "                    compress_ratio = 0.2\n",
    "                else:\n",
    "                    compress_ratio = 0.05\n",
    "\n",
    "                extractive_summary = summarize(preprocess_text, word_count=300)\n",
    "                file_data_sent = {'Extractive Summary': str(extractive_summary)}\n",
    "\n",
    "        elif (task == \"Topic\"):\n",
    "            if topic == \"PEP\":\n",
    "                word_list = ['pep', 'politically exposed person', 'political']\n",
    "                topic_info = sentence_finder(preprocess_text, word_list)\n",
    "            elif topic == \"Terrorist Financing\":\n",
    "                word_list = ['terrorist financing','counter-terrorist financing', 'financing of terrorism','terrorism financing','terror funding']\n",
    "                topic_info = sentence_finder(preprocess_text, word_list)\n",
    "            else:\n",
    "                word_list = ['aml','anti-money laundering', 'anti money laundering']\n",
    "                topic_info = sentence_finder(preprocess_text, word_list)\n",
    "\n",
    "            file_data_sent = {'Topic Information': str(topic_info)}\n",
    "\n",
    "\n",
    "        else:\n",
    "            if (int(parsedPDF[\"metadata\"]['xmpTPg:NPages']) < 40):\n",
    "\n",
    "                tokenizer=T5Tokenizer.from_pretrained(basePath + 'models/t5_small)\n",
    "                model = T5ForConditionalGeneration.from_pretrained(basePath + 'models/t5_small')\n",
    "\n",
    "                device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "                t5_prepared_Text= \"question:\" + question + \"context:\" + preprocess.text\n",
    "                tokenized_ text = tokenizer.encode(t5_prepared_Text, return_tensors=\"pt\").to(device)\n",
    "                summary_ids = model.generate(tokenized_text,num_beams=4, no_repeat_ngram_size=2, min_length=50,max_ length=300, early_stopping=False)\n",
    "                answer = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "                file_data_sent= {('Answer':str(answer)}\n",
    "            else:\n",
    "                file_data_sent = {'Answer': str('Document too big to process!!!')}\n",
    "\n",
    "                      \n",
    "        st = datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d_%H-%M-%S')\n",
    "        print（'************** Execution completed::', st)\n",
    "        print（'************** Total Execution completed::', time.time()- start)\n",
    "        final_filename = final_filename + \".docx\"\n",
    "\n",
    "        return jsonify(file_data_sent)\n",
    "I      \n",
    "doc_classifier_parser = api.parser()\n",
    "doc_classifier_parser.add_argument('file', location='files', required-True, type=FileStorage, help='Upload the PDF file')\n",
    "\n",
    "@api.route('/api/v1/DocClassifier')\n",
    "@api.response(500,'An unexpected error occured.')\n",
    "class IGAD_DocClassifier(Resource):\n",
    "      @api.expect(doc_classifier_parser)\n",
    "      def post(self):\n",
    "            config=configparser.ConfigParser()\n",
    "            config.read('config.ini')\n",
    "            basePath = config['DEFAULT'][\"basePath']\n",
    "            #tika.TikaJarPath = basePath + 'tika'\n",
    "\n",
    "            uploaded_file = request.files['file'] # This is FileStorage instance\n",
    "            st = datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d_%H-%M-%S')\n",
    "            start = time.time()\n",
    "            print（'*************** Execution started::'， st）\n",
    "            pos = uploaded_file.filename.rfind(\".\")\n",
    "                                         \n",
    "            final_filename = uploaded_file,filename[:pos].replace(\" \", \"_\") + \"_\" + st\n",
    "            UPLOAD_FOLDER = basePath + 'input/'\n",
    "            uploaded_file.save(os.path.join(UPLOAD_FOLDER, final_filename + uploaded_file.filename[pos:]))\n",
    "            parsedPDF = parser.from_file(os.path.join(UPLOAD_FOLDER, final_filename + uploaded_file.filename[pos:]))\n",
    "            metadata = parsedPDF [\"metadata\"]\n",
    "            pdf_content = str(parsedPDF[\"content\"].encode('ascii', errors='ignore'))\n",
    "            pages_txt = []\n",
    "            PDF_pages = parser.from_file(os.path.join(UPLOAD_FOLDER, final_filename + uploaded_file.filename[pos:]), xmlContent=True)\n",
    "            xhtml_data = BeautifulSoup(PDF_pages['content'])\n",
    "            for i, content in enumerate(xhtml_data.find_all('div', attrs=('class': 'page'})):\n",
    "                _buffer = StringIO()\n",
    "                _buffer.write(str(content))\n",
    "                parsed_content = parser.from_buffer(_buffer.getvalue())\n",
    "                text = parsed_content['content'].strip()\n",
    "                pages_txt.append(str(text.encode('ascii', errors='ignore')).strip().replace(\"\\\\n\", \"\"))\n",
    "\n",
    "            preprocess_text = pdf_content.strip().replace(\"\\\\n\",\"\")\n",
    "            DetectorFactory.seed = 2\n",
    "            lang_detected =str(detect_langs(preprocess_text))\n",
    "            r = Rake()\n",
    "            r.extract_keywords_from_text(preprocess_text)\n",
    "            keyphrases = r.get_ranked_phrases()[0:5]\n",
    "            keywords = str(gensim.summarization.keywords(preprocess_text, words=20, lemmatize=True).split('\\n'))\n",
    "            nlp = spacy.load(\"en_core_web_sm\")\n",
    "            nlp.max_length = 4000000\n",
    "            spacy_doc= nlp(preprocess_text)                                    \n",
    "            ner_lst =[]\n",
    "            for ent in spacy_doc.ents:\n",
    "                if (ent.label_in ['PERSON', 'ORG', 'GPE', 'EVENT', 'LAW', 'MONEY']):\n",
    "                        ner_lst.append((ent.text, ent.label_))\n",
    "\n",
    "            ner_data = list(set(ner_lst))\n",
    "\n",
    "            ner_Output = {}\n",
    "            for x, y in ner_data:\n",
    "                if y in ner_Output:\n",
    "                    ner_Output[y].append((x))\n",
    "                else:\n",
    "                    ner_Output[y] =[(x)]\n",
    "            I\n",
    "            imp_words = pages_txt[0] + ' ' + uploaded_file.filename[:pos]\n",
    "            imp_spacy_doc = nlp(imp_words)\n",
    "\n",
    "            imp_ner_lst = []\n",
    "            for ent in imp_spacy_doc.ents:\n",
    "                if(ent.label.in ['PERSON', 'NORP','FAC', 'ORG', 'GPE', 'LOC', 'PRODUCT', 'EVENT', 'WORK_OF_ART', 'LAW','DATE','MONEY']):\n",
    "                    imp_ner_lst.append((ent.text, ent.label_))\n",
    "\n",
    "            imp_ner_data = list(set(imp_ner_lst))\n",
    "\n",
    "            imp_ner_Output = {}\n",
    "            for x,y in imp_ner_data:\n",
    "                if y in imp_ner_Output:\n",
    "                    imp_ner_Output[y].append((x))\n",
    "                else:\n",
    "                    imp_ner_Output[y]= [(x)]\n",
    "                            \n",
    "            clf = pickle.load(open(basePath + 'docs/pyss3/pyss3_v2.pkl', 'rb'))\n",
    "            class_pred = clf.classify_label(preprocess_text)\n",
    "            c_pred = clf.classify(preprocess_text,sort=True)\n",
    "                            \n",
    "            for index,item in enumerate(c_pred):\n",
    "                Itemlist = list(item)\n",
    "                if itemlist[0] == 0:\n",
    "                    itemlist[0] = 'ABC'\n",
    "                elif itemlist[0] == 1:\n",
    "                    itemlist [0] = 'AML'\n",
    "                elif itemlist[0] == 2:\n",
    "                    itemlist [0] = 'EMBARGO'\n",
    "                elif itemlist[0] == 3:\n",
    "                    itemlist[0] = 'GDPR'\n",
    "                elif itemlist[0] == 4:\n",
    "                    itemlist[0] = 'KYC'\n",
    "                elif itemlist[0] == 5:\n",
    "                    itemlist[0] = 'unknown'\n",
    "\n",
    "                item= tuple(itemlist)\n",
    "                c_pred[index] = item\n",
    "\n",
    "            stopwords = set(STOPWORDS)\n",
    "            stopwords.update([\"should\", \"now\", \"will\", \"include\", \"may be\", \"ha\", \"Where\"])\n",
    "\n",
    "            mydoc = docx.Document()\n",
    "            mydoc.add_heading('IGAD Document Analyzer', 0)\n",
    "\n",
    "            mydoc.add_paragraph('Analysis for document ::' + str(uploaded_file.filename)) \n",
    "            mydoc.add_paragraph('File character length ::' + str(len(preprocess_text))) \n",
    "            mydoc.add_paragraph('Number of pages ::' + str(parsedPDF[\"metadata\"]['xmpTPg:NPages']))\n",
    "            if 'Creation-Date' in parsedPDF[\"metadata\"]:\n",
    "                    mydoc.add_paragraph('Creation Date::' + str(parsedPDF[\"metadata\"]['Creation-Date']))\n",
    "            if 'CreationDate' in parsedPDF[\"metadata\"]:\n",
    "                    mydoc.add_paragraph('Creation Date::' + str(parsedPDF[\"metadata\"][\"CreationDate\"]))\n",
    "            if 'Author' in parsedPDF[\"metadata\"]:\n",
    "                    mydoc.add_paragraph('Author ::' + str(parsedPDF[\"metadata\"]['Author']))\n",
    "            mydoc.add_paragraph('Language detected ::' + str(lang_detected)) \n",
    "            mydoc.add_paragraph('Document Class detected::' + str(class_pred))\n",
    "            mydoc.add_paragraph('Document Class Confidence Values ::' + str(c_pred)) \n",
    "\n",
    "            mydoc.add_heading('File metadata::', level=2)\n",
    "            mydoc.add_paragraph(str(metadata))\n",
    "\n",
    "            mydoc.add_heading('Key Phrases ::', level=2)\n",
    "            mydoc.add_paragraph(str(keyphrases))\n",
    "\n",
    "            mydoc.add_heading('Keywords::', level=2)\n",
    "            mydoc.add_paragraph(str(keywords))\n",
    "\n",
    "            mydoc.add_heading('Important Extracted Entities ::', level=2) \n",
    "            mydoc.add_paragraph(str(imp_ner_Output))\n",
    "\n",
    "            mydoc.add_heading('All Extracted Entities ::', level=2)\n",
    "            mydoc.add_paragraph(str(ner_Output))\n",
    "\n",
    "            file_data_sent = {'Input file': str(uploaded_file.filename), 'Metadata': str(metadata),'Document Class': str(class_pred), 'Document Class Confidence Values': str(c_pred),'Word Cloud': str(wordcloud_sg.words_), 'Document length': len(preprocess_text),'Language detected': str(lang_detected),'Number of pages': str(parsedPDF[\"metadata\"]['xmpTPg:NPages']),'Keywords': str(keywords), 'Important Extracted Entities': str(imp_ner_Output),'All Named Entities': str(ner_Output), 'Key Phrases': str(keyphrases)}\n",
    "            mydoc.save(os.path.join(UPLOAD_FOLDER, final_filename + \".docx\"))\n",
    "\n",
    "            st = datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d_%H-%M-%S')\n",
    "            print（'************Execution completed ::'，st）\n",
    "            print('*************Total execution time::', time.time() -start)\n",
    "            final_filename = final_filename + \".docx\"\n",
    "\n",
    "            return jsonify(file_data_sent)\n",
    "\n",
    "compliance_parser = api.parser()\n",
    "compliance_parser.add_argument('Message', type='string', required=True, help='Enter data to be analysed')\n",
    "\n",
    "@api.route('/api/v1/Compliance')\n",
    "@api.response(500,'An unexpected error occured, Please contact administrator.')\n",
    "class IGAD_Compliance(Resource):\n",
    "    @api.expect(compliance_parser)\n",
    "    def post(self):\n",
    "        message = request.args.get('Message')\n",
    "\n",
    "        parsed_text = CommonRegex(message)\n",
    "        pii_detected = defaultdict(list)\n",
    "        if(len(parsed_text.emails) > 0):\n",
    "            pii_detected['Email'].append(str(parsed_text.emails))\n",
    "        if(len(parsed_text.ipv6s)>0 | len(parsed_texts.ips)>0):\n",
    "            pii_detected['IP'].append(str(parsed_text.ips))\n",
    "        if(len(parsed_text.street_addresses) › 0):\n",
    "            pii_detected['Address'].append(str(parsed_text.street_addresses))\n",
    "        if (len(parsed_text.phones) > 0):\n",
    "            pii_detected['Phones'].append(str(parsed_text.phones))\n",
    "\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "        nlp.max_length= 4000000\n",
    "        spacy_doc = nlp(message)\n",
    "\n",
    "        ner_person = []\n",
    "        ner_org = []\n",
    "        ner_date = []\n",
    "        ner_money = []\n",
    "        ner_norp = []\n",
    "        ner_gpe = []\n",
    "\n",
    "        for ent in spacy_doc.ents:\n",
    "            if(ent.label_ == 'PERSON'):\n",
    "                ner_person.append(ent.text)\n",
    "            if(ent.label_ == 'ORG'): \n",
    "                ner_org-append(ent.text)\n",
    "            if(ent.label == 'DATE'):\n",
    "                ner_date.append(ent.text)\n",
    "            if(ent.label_ == 'MONEY'):\n",
    "                ner_money.append(ent.text)\n",
    "            if(ent.label_ == 'NORP'): \n",
    "                ner_norp.append(ent.text)\n",
    "            if(ent.label_ == 'GPE'): \n",
    "                ner_gpe.append(ent.text)\n",
    "\n",
    "        if(len(ner_person) > 0):\n",
    "            pii_detected['Person'].append(str(set(ner_person)))\n",
    "        if(len(ner_org) > 0):\n",
    "            pii_detected['Organisation'].append(str(set(ner_org)))\n",
    "        if(len(ner_date) > 0):\n",
    "            pii_detected ['Date'].append(str(set(ner_date)))\n",
    "        if(len(ner_norp) > 0):\n",
    "            pii_detected['Nationality'].append(str(set(ner_norp)))\n",
    "        if(len(ner_gpe) > 0):\n",
    "            pii_detected['Location' ].append(str(set(ner_gpe)))\n",
    "        if(not(bool(pii_ detected))):\n",
    "            pii_detected = 'NO'\n",
    "        else:\n",
    "            pii_detected = json.dumps(pii_detected)\n",
    "\n",
    "        sensitive_info = defaultdict(list)\n",
    "        if(len(parsed_text.credit_cards) >0 ):\n",
    "            sensitive_info['Credit_Card'].append(str(parsed_text.credit_cards))\n",
    "        if(len(ner_money) > 0):\n",
    "            sensitive_info['Money'].append(str(ner_money))\n",
    "\n",
    "        if(not(bool(sensitive_info))):\n",
    "            sensitive_info = 'NO'\n",
    "        else:\n",
    "            sensitive_info = json.dumps(sensitive_info)\n",
    "\n",
    "        if predict([message]) == [0]:\n",
    "            Offensive = 'NO'\n",
    "        else:\n",
    "            Offensive = 'Alert!!! Objectionable language detected.\\n'\n",
    "\n",
    "        file_data_sent = {'PII':pii_detected, 'Objectionable': Offensive, 'Sensitive': sensitive_info}\n",
    "\n",
    "        return jsonify(file_data_sent)\n",
    "                            \n",
    "                            \n",
    "anomaly_parser = api.parser()\n",
    "anomaly_parser.add_argument('file', location='files', required=True, type=FileStorage, help='Upload the file:')\n",
    "\n",
    "@api.route('/api/v1/Anomaly_detection')\n",
    "@api.response(500, 'An unexpected error occured, Please contact administrator.')\n",
    "class SG_AI_API_UpLoad(Resource):\n",
    "    @api.expect(anomaly_parser)\n",
    "    def post(self):\n",
    "        config = configparser.ConfigParser()\n",
    "        config.read('config.ini')\n",
    "        basePath = config['DEFAULT']['basePath']\n",
    "        uploaded_file = request.files['file'] # This is Filestorage instance\n",
    "                            \n",
    "        st = datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d_%H-&M-&S')\n",
    "                            \n",
    "        pos = uploaded_file. filename.rfind(\".\")\n",
    "        UPLOAD_FOLDER = basePath + 'input/'\n",
    "\n",
    "        final_filename= uploaded_file.filename[:pos].replace(\" \",\"_\") + \"_\" + st+ uploaded_file.filename[pos:]\n",
    "        uploaded_file.save(os.path.join(UPLOAD_FOLDER, final_filename))\n",
    "                            \n",
    "        df = read_csv(UPLOAD_FOLDER + final_filename, header=None)\n",
    "                            \n",
    "        iso = IsolationForest(contamination=0.1)\n",
    "        iso.fit(df)\n",
    "        iso_pred = iso.predict(df)\n",
    "        knn = KNN()\n",
    "        knn.fit(df)\n",
    "        knn_pred = knn.predict(df)\n",
    "        oc_svm = svm.OneClassSVM()\n",
    "        oc_svm.fit(df)\n",
    "        oc_pred = oc_svm.predict(df)\n",
    "        outliers = df[(iso_pred == -1) & (knn_pred == 1) & (oc_ pred == -1)]\n",
    "        bl = benfordslaw(alpha=0.05)\n",
    "        benford_array = []\n",
    "        for i in range(0, df.shape[1]):\n",
    "            results = bl.fit(df[[i]] .values)\n",
    "            if(results.get('P') <= 0.05): \n",
    "                benford_array.append(i + 1)\n",
    "                            \n",
    "        benford_law = 'Anomaly detected in following column numbers as per Benford law:' + str(benford_array)\n",
    "\n",
    "        data_sent = {'BenfordLaw': benford_law, 'Outliers': outliers.to_json()}\n",
    "        \n",
    "        return jsonify(data_sent)\n",
    "if__name__== '__main__':\n",
    "        igad_application.run(host='0,0.0.0', port=5050, debug=True, threaded=True, use_reloader=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d03f52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeea5dba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4652362",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
